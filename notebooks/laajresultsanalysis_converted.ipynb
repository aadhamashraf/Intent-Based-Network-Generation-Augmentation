{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadhamashraf/Intent-Based-Network-Generation-Augmentation/blob/main/laajresultsanalysis_converted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af6d5a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac4bbab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ac4bbab",
        "outputId": "10ecf601-609d-469d-a6ce-92b4cb80c309"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "import pprint\n",
        "\n",
        "df = pd.read_excel(\"gen_dataset_evaluation.xlsx\" , sheet_name=1)\n",
        "df.columns = df.columns.str.strip()\n",
        "df[\"Intent Type\"] = df[\"Intent Type\"].str.strip()\n",
        "df = df[df[\"Intent Type\"] != \":---\"]\n",
        "df['Description'] = df['Description'].str.replace('\"', '').str.strip()\n",
        "df['Description'] = df['Description'].str.replace(\"“\", \"\").str.replace(\"”\", \"\").str.strip()\n",
        "\n",
        "df.columns\n",
        "\n",
        "numericalValues  = ['Standards Alignment / Correctness (1-5)' , 'Complexity (1-5)' , 'Readability (1-5)' , 'Relevance (1-5)' , 'Coherence (1-5)' , 'Consistency (1-5)' , 'Completeness (1-5)' , 'Overall Score']\n",
        "\n",
        "for i in numericalValues:\n",
        "    df[i] = pd.to_numeric(df[i], errors='coerce')\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xI00L5xQEF06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "xI00L5xQEF06",
        "outputId": "93906c70-f226-4af7-9c21-992e0a14ff37"
      },
      "outputs": [],
      "source": [
        "dfGen = pd.read_excel(\"gen_dataset_evaluation.xlsx\" , sheet_name=3)\n",
        "dfGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a5cc18",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reset = df.reset_index(drop=True)\n",
        "dfGen_reset = dfGen.reset_index(drop=True)\n",
        "\n",
        "mask = df_reset[\"Description\"] == dfGen_reset[\"Description\"]\n",
        "\n",
        "df_matched = df_reset[mask].reset_index(drop=True)\n",
        "dfGen_matched = dfGen_reset[mask].reset_index(drop=True)\n",
        "\n",
        "df_mismatches = df_reset[~mask].reset_index(drop=True)\n",
        "dfGen_mismatches = dfGen_reset[~mask].reset_index(drop=True)\n",
        "\n",
        "mismatches_combined = pd.DataFrame({\n",
        "    \"df_Description\": df_mismatches[\"Description\"],\n",
        "    \"dfGen_Description\": dfGen_mismatches[\"Description\"]\n",
        "})\n",
        "\n",
        "print(f\"Original length: {len(df)}\")\n",
        "print(f\"Matched length: {len(df_matched)}\")\n",
        "print(f\"Mismatched length: {len(df_mismatches)}\")\n",
        "\n",
        "print(\"\\nFirst 10 mismatches:\")\n",
        "print(mismatches_combined.head(10))\n",
        "print((df_matched[\"Description\"] != dfGen_matched[\"Description\"]).any())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cad4ccc5",
      "metadata": {
        "id": "cad4ccc5"
      },
      "source": [
        "# **Low-Score Distribution by Evaluation Dimension**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xtVJ6RzYrGuV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtVJ6RzYrGuV",
        "outputId": "ebfceb8e-9bd4-413f-a26a-1e335619277f"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for col in numericalValues:\n",
        "    total = df[col].notna().sum()\n",
        "    below3 = (df[col] < 4).sum()\n",
        "    perc = (below3 / total * 100) if total > 0 else 0\n",
        "    results.append([col, below3, total, round(perc, 2)])\n",
        "\n",
        "summary_df = pd.DataFrame(results, columns=[\"Column\", \"Count < 4\", \"Total\", \"Percentage %\"])\n",
        "print(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvg0As4-rIfo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "wvg0As4-rIfo",
        "outputId": "ff9a6b66-0ca5-4ae2-d73e-404e60ed2ee9"
      },
      "outputs": [],
      "source": [
        "all_summaries = []\n",
        "\n",
        "for col in numericalValues:\n",
        "    intent_counts = (df[df[col] < 4].groupby(\"Intent Type\").size().reindex(df[\"Intent Type\"].unique(), fill_value=0) )\n",
        "\n",
        "    total = df[col].notna().sum()\n",
        "    below4 = (df[col] < 4).sum()\n",
        "    perc = (below4 / total * 100) if total > 0 else 0\n",
        "\n",
        "    row = {\"Metric\": col, \"Total <4\": below4, \"% <4\": round(perc, 2)}\n",
        "    row.update(intent_counts.to_dict())\n",
        "    all_summaries.append(row)\n",
        "\n",
        "intent_summary_df = pd.DataFrame(all_summaries)\n",
        "intent_summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ced33cd",
      "metadata": {
        "id": "6ced33cd"
      },
      "outputs": [],
      "source": [
        "bins = [0, 1, 2, 3, 4]\n",
        "labels = [\"0–1\", \"1–2\", \"2–3\", \"3–4\"]\n",
        "\n",
        "intent_summary_dfs = {}\n",
        "\n",
        "for intent in df[\"Intent Type\"].unique():\n",
        "    all_summaries = []\n",
        "\n",
        "    df_intent = df[df[\"Intent Type\"] == intent]\n",
        "\n",
        "    for col in numericalValues:\n",
        "        total = df_intent[col].notna().sum()\n",
        "\n",
        "        binned = pd.cut(df_intent[col], bins=bins, labels=labels, right=False)\n",
        "        range_counts = binned.value_counts().reindex(labels, fill_value=0)\n",
        "        range_perc = (range_counts / total * 100).round(2) if total > 0 else 0\n",
        "\n",
        "        row = {\"Metric\": col, \"Total\": total}\n",
        "        for label in labels:\n",
        "            row[f\"Count {label}\"] = range_counts[label]\n",
        "            row[f\"% {label}\"] = range_perc[label]\n",
        "\n",
        "        all_summaries.append(row)\n",
        "\n",
        "    intent_summary_dfs[intent] = pd.DataFrame(all_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DeFahpssnVSi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "DeFahpssnVSi",
        "outputId": "160e0ab4-7f98-4bf2-b0b9-46d1cb2bc1e4"
      },
      "outputs": [],
      "source": [
        "deployment_summary = intent_summary_dfs.get('Deployment Intent')\n",
        "deployment_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IycmMtOanW4M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "IycmMtOanW4M",
        "outputId": "3af1e6c2-ecb6-40d2-8b80-472d60af8c24"
      },
      "outputs": [],
      "source": [
        "feasability_check_summary = intent_summary_dfs.get('Intent Feasibility Check')\n",
        "feasability_check_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NCHK-6AanYVk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "NCHK-6AanYVk",
        "outputId": "55c5b623-88b2-4b43-e3e1-f5d2d1520599"
      },
      "outputs": [],
      "source": [
        "report_request_summary = intent_summary_dfs.get('Intent Report Request')\n",
        "report_request_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VwWLky8knaHc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "VwWLky8knaHc",
        "outputId": "1121728c-3062-439f-b17b-2c177e70041f"
      },
      "outputs": [],
      "source": [
        "modification_summary = intent_summary_dfs.get('Modification Intent')\n",
        "modification_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iFq8hzaenbd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "iFq8hzaenbd8",
        "outputId": "bfaca799-763f-40f9-ee43-3232ccd3a762"
      },
      "outputs": [],
      "source": [
        "regular_notification_request_summary = intent_summary_dfs.get('Regular Notification Request')\n",
        "regular_notification_request_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mmokkNLmndoj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "mmokkNLmndoj",
        "outputId": "464c0daf-20ce-4cd2-a51e-548d9ebac62e"
      },
      "outputs": [],
      "source": [
        "performance_assurance_summary = intent_summary_dfs.get('Performance Assurance Intent')\n",
        "performance_assurance_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c995ee",
      "metadata": {
        "id": "93c995ee"
      },
      "source": [
        "# **Intent Types Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2167eb6",
      "metadata": {
        "id": "c2167eb6"
      },
      "outputs": [],
      "source": [
        "justification_map = {\n",
        "    \"Standards Alignment / Correctness (1-5)\": \"Standards Justification\",\n",
        "    \"Complexity (1-5)\": \"Complexity Justification\",\n",
        "    \"Readability (1-5)\": \"Readability Justification\",\n",
        "    \"Relevance (1-5)\": \"Relevance Justification\",\n",
        "    \"Coherence (1-5)\": \"Coherence Justification\",\n",
        "    \"Consistency (1-5)\": \"Consistency Justification\",\n",
        "    \"Completeness (1-5)\": \"Completeness Justification\",\n",
        "}\n",
        "\n",
        "def get_intent_justification_dfs(df, intent_type, score_name, bins, labels):\n",
        "    \"\"\"\n",
        "    Split intent justifications into separate DataFrames per score range,\n",
        "    sorted by justification count.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df[\"Intent Type\"] = df[\"Intent Type\"].str.strip()\n",
        "\n",
        "    # Copy here to avoid SettingWithCopyWarning\n",
        "    df_intent = df[df[\"Intent Type\"] == intent_type].copy()\n",
        "\n",
        "    justification_col = justification_map.get(score_name)\n",
        "    if justification_col is None:\n",
        "        raise ValueError(f\"No justification column found for score '{score_name}'\")\n",
        "\n",
        "    # Bin the scores\n",
        "    df_intent[\"_range\"] = pd.cut(df_intent[score_name], bins=bins, labels=labels, right=False)\n",
        "\n",
        "    dfs = {}\n",
        "    for label in labels:\n",
        "        subset = df_intent[df_intent[\"_range\"] == label]\n",
        "        if subset.empty:\n",
        "            dfs[label] = pd.DataFrame(columns=[\"justification\", \"count\", \"indices\"])\n",
        "            continue\n",
        "\n",
        "        records = []\n",
        "        for just, group in subset.groupby(justification_col):\n",
        "            records.append({\n",
        "                \"justification\": just,\n",
        "                \"count\": len(group),\n",
        "                \"indices\": group.index.tolist()\n",
        "            })\n",
        "\n",
        "        dfs[label] = (\n",
        "            pd.DataFrame(records)\n",
        "            .sort_values(by=\"count\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "\n",
        "    return dfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1bd26a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_template_from_description(description):\n",
        "    \"\"\"\n",
        "    Given a description, return the template associated with the dfgen dataset.\n",
        "\n",
        "    Args:\n",
        "        description (str): The description to match.\n",
        "        df (pd.DataFrame): The DataFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "        str: The associated template, or None if not found.\n",
        "    \"\"\"\n",
        "    matching_row = dfGen[dfGen['Description'] == description]\n",
        "    return matching_row['BaseTemplate'].iloc[0]  if not matching_row.empty  else None \n",
        "    \n",
        "# test_description = \"Provision high-deterministic performance compute resources with 16 x86_64 cores and 26GB DDR5 memory for demanding network functions.\"\n",
        "# template = get_template_from_description(test_description)\n",
        "\n",
        "# print(f\"The template for description '{test_description}' is: {template}\") if template else print(f\"No template found for description: '{test_description}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf03bdde",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def analyze_justifications(df, subset):\n",
        "    \"\"\"\n",
        "    Walk through justifications, show associated descriptions/templates,\n",
        "    and compute unique template counts (per-justification and global).\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): The main dataframe containing 'Description'\n",
        "        subset (pd.DataFrame): Subset with columns ['justification', 'indices']\n",
        "    \n",
        "    Returns:\n",
        "        dict: {justification: Counter(templates)}\n",
        "        Counter: global template frequency\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    global_templates = []\n",
        "\n",
        "    for idx, row in subset.iterrows():\n",
        "        justification = row['justification']\n",
        "        indices = row['indices']\n",
        "        \n",
        "        print(f\"Justification: {justification}\")\n",
        "        print(f\"Appears in rows: {indices}\")\n",
        "        \n",
        "        templates = []\n",
        "        \n",
        "        for i in indices:\n",
        "            description = df.at[i, 'Description']\n",
        "            template = get_template_from_description(description)\n",
        "            templates.append(template)\n",
        "            global_templates.append(template)\n",
        "            \n",
        "            print(f\"  Description: {description}\")\n",
        "            print(f\"  Template: {template}\")\n",
        "        \n",
        "        # summarize unique templates per justification\n",
        "        template_counts = Counter(templates)\n",
        "        results[justification] = template_counts\n",
        "        \n",
        "        print(\"\\nUnique templates and counts:\")\n",
        "        for tpl, cnt in template_counts.items():\n",
        "            print(f\"  {tpl}: {cnt}\")\n",
        "        \n",
        "        print(\"-\" * 60 + \"\\n\")\n",
        "    \n",
        "    # final global summary\n",
        "    global_counts = Counter(global_templates)\n",
        "    print(\"=== Final Global Template Counts ===\")\n",
        "    for tpl, cnt in global_counts.items():\n",
        "        print(f\"{tpl}: {cnt}\")\n",
        "    \n",
        "    return results, global_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e853cde",
      "metadata": {
        "id": "2e853cde"
      },
      "source": [
        "## **Deployment Intent**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wyAXmKBunlgf",
      "metadata": {
        "id": "wyAXmKBunlgf"
      },
      "source": [
        "### **Complexity Measure (1-5)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RghiP7-Zohpr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RghiP7-Zohpr",
        "outputId": "a54817e3-c3b2-43ff-bbe8-cf6612249f52"
      },
      "outputs": [],
      "source": [
        "deployment_complexity = get_intent_justification_dfs(df, \"Deployment Intent\", \"Complexity (1-5)\", bins, labels)\n",
        "deployment_complexity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f5e197",
      "metadata": {},
      "source": [
        "#### **2-3 Range**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YoV0hMG4DVJe",
      "metadata": {
        "id": "YoV0hMG4DVJe"
      },
      "outputs": [],
      "source": [
        "deployment_complexity_2_3 = deployment_complexity['2–3']\n",
        "deployment_complexity_2_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3bdff4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a3bdff4",
        "outputId": "563a1ed9-40e3-428f-e2ff-59f6d0a7222b"
      },
      "outputs": [],
      "source": [
        "deployment_complexity_2_3['justification']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F4A_gm4YBLR9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4A_gm4YBLR9",
        "outputId": "383d6fc1-25a2-45ad-ca3c-18a06dd81b73"
      },
      "outputs": [],
      "source": [
        "results, global_counts = analyze_justifications(df, deployment_complexity_2_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f636b25f",
      "metadata": {},
      "source": [
        "## **Aggregating Other Intents with it's scores and range**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434b6b24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "def sanitize_sheet_name(name: str) -> str:\n",
        "    \"\"\"Clean Excel sheet names: replace invalid chars, truncate to 31 chars\"\"\"\n",
        "    name = re.sub(r'[\\[\\]\\:\\*\\?\\/\\\\]', '_', name)\n",
        "    return name.strip()[:31]\n",
        "\n",
        "def analyze_intent_scores_ranges(df, dfGen, intent, bins, labels, justification_map, output_dir=\"./exports\", top_n=20):\n",
        "    \"\"\"\n",
        "    Analyze a single intent type:\n",
        "    - Range sheets\n",
        "    - Justification matrix sheets\n",
        "    - Score summary sheets\n",
        "    - Global summary (per template, per range)\n",
        "    - Top templates\n",
        "    - Range × Template counts sheets\n",
        "    Export results in Excel and JSON.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    excel_path = os.path.join(output_dir, f\"{intent}.xlsx\")\n",
        "    json_path = os.path.join(output_dir, f\"{intent}.json\")\n",
        "\n",
        "    global_template_counts = Counter()\n",
        "    json_results = {}\n",
        "\n",
        "    all_templates = pd.unique(dfGen[\"BaseTemplate\"])\n",
        "    global_range_counts = pd.DataFrame(0, index=all_templates, columns=labels)\n",
        "\n",
        "    with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as writer:\n",
        "        workbook = writer.book\n",
        "        scores = list(justification_map.keys())\n",
        "\n",
        "        for score in scores:\n",
        "            print(f\"-- Score: {score} --\")\n",
        "            try:\n",
        "                intent_score_dfs = get_intent_justification_dfs(df, intent, score, bins, labels)\n",
        "            except Exception as e:\n",
        "                print(f\"  Skipping {score} for {intent} due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "            score_json = {}\n",
        "            all_ranges_results = []\n",
        "\n",
        "            for rng in labels:\n",
        "                subset = intent_score_dfs[rng]\n",
        "                if subset.empty:\n",
        "                    continue\n",
        "\n",
        "                results, global_counts = analyze_justifications(df, subset)\n",
        "                global_template_counts.update(global_counts)\n",
        "                all_ranges_results.append((rng, results, global_counts))\n",
        "\n",
        "                safe_range_sheet = sanitize_sheet_name(f\"{score}_{rng}\")\n",
        "                range_records = []\n",
        "                for justification, counts in results.items():\n",
        "                    for template, count in counts.items():\n",
        "                        range_records.append({\n",
        "                            \"Justification\": justification,\n",
        "                            \"Template\": template,\n",
        "                            \"Count\": count\n",
        "                        })\n",
        "\n",
        "                        if template in global_range_counts.index:\n",
        "                            global_range_counts.loc[template, rng] += count\n",
        "                if range_records:\n",
        "                    pd.DataFrame(range_records).to_excel(writer, sheet_name=safe_range_sheet, index=False)\n",
        "\n",
        "                descriptions, templates = [], []\n",
        "                for idx_list in subset[\"indices\"]:\n",
        "                    for idx in idx_list:  \n",
        "                        desc = df.at[idx, \"Description\"]\n",
        "                        tpl_match = dfGen.loc[dfGen[\"Description\"] == desc, \"BaseTemplate\"]\n",
        "                        tpl = tpl_match.iloc[0] if not tpl_match.empty else \"Unknown\"\n",
        "                        descriptions.append(desc)\n",
        "                        templates.append(tpl)\n",
        "\n",
        "                if descriptions and templates:\n",
        "                    justification_matrix = pd.DataFrame(\n",
        "                        0,\n",
        "                        index=pd.unique(descriptions),\n",
        "                        columns=pd.unique(templates)\n",
        "                    )\n",
        "                    for desc, tpl in zip(descriptions, templates):\n",
        "                        justification_matrix.loc[desc, tpl] += 1\n",
        "\n",
        "                    safe_just_sheet = sanitize_sheet_name(f\"{score}_{rng}_matrix\")\n",
        "                    justification_matrix.to_excel(writer, sheet_name=safe_just_sheet)\n",
        "\n",
        "                    worksheet = writer.sheets[safe_just_sheet]\n",
        "                    fmt = workbook.add_format({'bg_color': '#FFC7CE'})\n",
        "                    worksheet.conditional_format(\n",
        "                        1, 1, len(justification_matrix), len(justification_matrix.columns),\n",
        "                        {'type': 'cell', 'criteria': '>', 'value': 1, 'format': fmt}\n",
        "                    )\n",
        "\n",
        "                score_json[rng] = {\n",
        "                    \"range_template_counts\": global_range_counts.loc[:, rng].to_dict(),\n",
        "                    \"justifications\": {j: dict(c) for j, c in results.items()}\n",
        "                }\n",
        "\n",
        "            if all_ranges_results:\n",
        "                summary_df = pd.DataFrame([\n",
        "                    {\n",
        "                        \"Range\": rng,\n",
        "                        \"Unique Justifications\": len(results),\n",
        "                        \"Total Template Occurrences\": sum(global_counts.values())\n",
        "                    }\n",
        "                    for rng, results, global_counts in all_ranges_results\n",
        "                ])\n",
        "                safe_summary_sheet = sanitize_sheet_name(f\"{score}_summary\")\n",
        "                summary_df.to_excel(writer, sheet_name=safe_summary_sheet, index=False)\n",
        "\n",
        "            safe_counts_sheet = sanitize_sheet_name(f\"{score}_range_template_counts\")\n",
        "            global_range_counts.to_excel(writer, sheet_name=safe_counts_sheet)\n",
        "\n",
        "            json_results[score] = score_json\n",
        "\n",
        "        global_range_counts[\"Total Count\"] = global_range_counts.sum(axis=1)\n",
        "        global_range_counts_sorted = global_range_counts.sort_values(by=\"Total Count\", ascending=False)\n",
        "        global_range_counts_sorted.to_excel(writer, sheet_name=\"Global_Summary\", index=True)\n",
        "\n",
        "        top_df = global_range_counts_sorted.head(top_n)\n",
        "        top_df.to_excel(writer, sheet_name=\"Top_Templates\", index=True)\n",
        "\n",
        "        json_results[\"global_summary\"] = global_range_counts_sorted.to_dict(orient=\"index\")\n",
        "        json_results[\"top_templates\"] = {k: v for k, v in global_range_counts_sorted[\"Total Count\"].head(top_n).items()}\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(json_results, f, indent=2)\n",
        "\n",
        "    print(f\"\\n=== Finished analysis for '{intent}' ===\")\n",
        "    print(f\"Excel exported to: {excel_path}\")\n",
        "    print(f\"JSON exported to: {json_path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730801d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Intent Type'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21802d91",
      "metadata": {},
      "source": [
        "### **Deployment Intent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4140784b",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_intent_scores_ranges(\n",
        "    df=df,\n",
        "    dfGen=dfGen,\n",
        "    intent=\"Deployment Intent\",\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    justification_map=justification_map,\n",
        "    output_dir=\"./exports\",\n",
        "    top_n=20  # top 20 templates shown in summary\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ce78c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "# Load the JSON results from your function\n",
        "intent = \"YourIntentName\"\n",
        "json_path = f\"./exports/{intent}.json\"\n",
        "\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Example: visualize range counts for a specific score\n",
        "score = list(data.keys())[0]  # pick the first score\n",
        "range_counts = data[score][\"range_template_counts\"]\n",
        "\n",
        "df_range = pd.DataFrame.from_dict(range_counts, orient=\"index\")\n",
        "df_range.plot(kind=\"bar\", figsize=(12,6))\n",
        "plt.title(f\"{intent} - Template counts per range for {score}\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Templates\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e07dc59",
      "metadata": {},
      "source": [
        "### **Modification Intent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87aa3fa3",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_intent_scores_ranges(\n",
        "    df=df,\n",
        "    dfGen=dfGen,\n",
        "    intent=\"Modification Intent\",\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    justification_map=justification_map,\n",
        "    output_dir=\"./exports\",\n",
        "    top_n=20  # top 20 templates shown in summary\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5ab54b",
      "metadata": {},
      "source": [
        "### **Regular Notification Request Intent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b919a4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_intent_scores_ranges(\n",
        "    df=df,\n",
        "    dfGen=dfGen,\n",
        "    intent=\"Regular Notification Request\",\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    justification_map=justification_map,\n",
        "    output_dir=\"./exports\",\n",
        "    top_n=20  # top 20 templates shown in summary\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31fe5a22",
      "metadata": {},
      "source": [
        "### **Intent Report Request**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d3f083",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_intent_scores_ranges(\n",
        "    df=df,\n",
        "    dfGen=dfGen,\n",
        "    intent=\"Intent Report Request\",\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    justification_map=justification_map,\n",
        "    output_dir=\"./exports\",\n",
        "    top_n=20  # top 20 templates shown in summary\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e16dcc2",
      "metadata": {},
      "source": [
        "### **Intent Feasibility Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1305f85",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_intent_scores_ranges(\n",
        "    df=df,\n",
        "    dfGen=dfGen,\n",
        "    intent=\"Intent Feasibility Check\",\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    justification_map=justification_map,\n",
        "    output_dir=\"./exports\",\n",
        "    top_n=20  # top 20 templates shown in summary\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba1b83",
      "metadata": {},
      "source": [
        "### **Performance Assurance Intent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1145867d",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_intent_scores_ranges(\n",
        "    df=df,\n",
        "    dfGen=dfGen,\n",
        "    intent=\"Performance Assurance Intent\",\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    justification_map=justification_map,\n",
        "    output_dir=\"./exports\",\n",
        "    top_n=20  # top 20 templates shown in summary\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
