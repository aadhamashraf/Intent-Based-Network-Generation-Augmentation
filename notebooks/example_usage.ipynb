{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent-Based Network Generation Augmentation Toolkit\n",
    "\n",
    "This notebook demonstrates the usage of the Intent-Based Network Generation Augmentation toolkit for creating sophisticated 3GPP network intent datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join('..', 'src'))\n",
    "\n",
    "from Intents_Generators.Advanced3GPPIntentGenerator import Advanced3GPPIntentGenerator\n",
    "from Intents_Generators.Constants_Enums import IntentType, Priority\n",
    "from Evaluation.evaluation_metric import DataEvaluator\n",
    "from augmentation_utils import paraphrase, back_translate, synonym_augment\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Intent Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "generator = Advanced3GPPIntentGenerator(use_llm_synthesis=False)\n",
    "\n",
    "# Generate sample intents\n",
    "print(\"Generating 10 sample intents...\")\n",
    "intents = generator.generate_batch(10)\n",
    "\n",
    "print(f\"Generated {len(intents)} intents\")\n",
    "print(\"\\nSample intents:\")\n",
    "for i, intent in enumerate(intents[:3], 1):\n",
    "    print(f\"\\n{i}. Type: {intent.intent_type}\")\n",
    "    print(f\"   Description: {intent.description[:100]}...\")\n",
    "    print(f\"   Priority: {intent.priority}\")\n",
    "    print(f\"   Location: {intent.location}\")\n",
    "    print(f\"   Network Slice: {intent.network_slice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze intent types\n",
    "intent_types = [intent.intent_type for intent in intents]\n",
    "priorities = [intent.priority for intent in intents]\n",
    "complexities = [intent.metadata.get('technical_complexity', 5) for intent in intents]\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Intent types distribution\n",
    "pd.Series(intent_types).value_counts().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Intent Types Distribution')\n",
    "axes[0].set_xlabel('Intent Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Priority distribution\n",
    "pd.Series(priorities).value_counts().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Priority Distribution')\n",
    "axes[1].set_xlabel('Priority')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Complexity distribution\n",
    "axes[2].hist(complexities, bins=5, alpha=0.7)\n",
    "axes[2].set_title('Technical Complexity Distribution')\n",
    "axes[2].set_xlabel('Complexity Level')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total intents: {len(intents)}\")\n",
    "print(f\"Unique intent types: {len(set(intent_types))}\")\n",
    "print(f\"Average complexity: {sum(complexities)/len(complexities):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample intent for augmentation\n",
    "sample_intent = intents[0]\n",
    "original_text = sample_intent.description\n",
    "\n",
    "print(f\"Original text:\")\n",
    "print(f\"{original_text}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Apply different augmentation techniques\n",
    "augmentation_techniques = [\n",
    "    (\"Synonym Replacement\", synonym_augment),\n",
    "    # Note: Other techniques may require models to be loaded\n",
    "]\n",
    "\n",
    "for name, func in augmentation_techniques:\n",
    "    try:\n",
    "        augmented = func(original_text)\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"{augmented}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: Error - {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to different formats\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# CSV export\n",
    "csv_file = f\"notebook_intents_{timestamp}.csv\"\n",
    "generator.export_to_csv(intents, csv_file)\n",
    "print(f\"✓ Exported to CSV: {csv_file}\")\n",
    "\n",
    "# JSON export\n",
    "json_file = f\"notebook_intents_{timestamp}.json\"\n",
    "generator.export_to_json(intents, json_file)\n",
    "print(f\"✓ Exported to JSON: {json_file}\")\n",
    "\n",
    "# Load and display CSV data\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"\\nCSV Data Preview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample descriptions for evaluation\n",
    "sample_descriptions = [intent.description for intent in intents[:3]]\n",
    "\n",
    "print(\"Sample descriptions for evaluation:\")\n",
    "for i, desc in enumerate(sample_descriptions, 1):\n",
    "    print(f\"{i}. {desc[:100]}...\")\n",
    "\n",
    "# Try evaluation (may not work without LLM setup)\n",
    "try:\n",
    "    evaluator = DataEvaluator()\n",
    "    result = evaluator.evaluate_batch(sample_descriptions)\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    metrics = result['overall_metrics']\n",
    "    print(f\"Overall Quality: {metrics.overall_quality:.2f}/10\")\n",
    "    print(f\"Technical Accuracy: {metrics.technical_accuracy:.2f}/10\")\n",
    "    print(f\"3GPP Compliance: {metrics.compliance_level:.2f}/10\")\n",
    "    print(f\"Research Value: {metrics.research_value:.2f}/10\")\n",
    "    \n",
    "    print(f\"\\nKey Insights:\")\n",
    "    for insight in result['batch_insights']:\n",
    "        print(f\"- {insight}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nEvaluation failed (expected without LLM setup): {e}\")\n",
    "    print(\"To enable evaluation, install and configure Ollama with Mistral model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text characteristics\n",
    "descriptions = [intent.description for intent in intents]\n",
    "text_lengths = [len(desc) for desc in descriptions]\n",
    "word_counts = [len(desc.split()) for desc in descriptions]\n",
    "\n",
    "# Create analysis plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0, 0].hist(text_lengths, bins=10, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Description Length Distribution')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(word_counts, bins=10, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "axes[0, 1].set_xlabel('Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Complexity vs Priority\n",
    "priority_mapping = {'LOW': 1, 'MEDIUM': 2, 'HIGH': 3, 'CRITICAL': 4, 'EMERGENCY': 5}\n",
    "priority_numeric = [priority_mapping.get(p, 2) for p in priorities]\n",
    "\n",
    "axes[1, 0].scatter(complexities, priority_numeric, alpha=0.7, color='coral')\n",
    "axes[1, 0].set_title('Complexity vs Priority')\n",
    "axes[1, 0].set_xlabel('Technical Complexity')\n",
    "axes[1, 0].set_ylabel('Priority Level')\n",
    "\n",
    "# Network slice distribution\n",
    "network_slices = [intent.network_slice for intent in intents if intent.network_slice]\n",
    "slice_counts = pd.Series(network_slices).value_counts().head(5)\n",
    "slice_counts.plot(kind='bar', ax=axes[1, 1], color='gold')\n",
    "axes[1, 1].set_title('Top 5 Network Slices')\n",
    "axes[1, 1].set_xlabel('Network Slice')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText Analysis:\")\n",
    "print(f\"Average description length: {sum(text_lengths)/len(text_lengths):.1f} characters\")\n",
    "print(f\"Average word count: {sum(word_counts)/len(word_counts):.1f} words\")\n",
    "print(f\"Most common network slice: {slice_counts.index[0] if len(slice_counts) > 0 else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "import glob\n",
    "\n",
    "generated_files = glob.glob(f\"notebook_intents_{timestamp}.*\")\n",
    "print(f\"Generated files:\")\n",
    "for file in generated_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file)\n",
    "        print(f\"- {file} ({size} bytes)\")\n",
    "\n",
    "# Uncomment to clean up files\n",
    "# for file in generated_files:\n",
    "#     if os.path.exists(file):\n",
    "#         os.remove(file)\n",
    "#         print(f\"Removed {file}\")\n",
    "\n",
    "print(\"\\n✓ Notebook execution completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}